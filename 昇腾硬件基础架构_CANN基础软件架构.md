#### 昇腾硬件基础架构、CANN基础软件架构

一、 昇腾硬件的核心是**达芬奇（Da Vinci）架构**，专为高效的AI计算设计。主要由：

1. **AI Core**：这是执行矩阵和向量计算的核心单元 🧠，它通过[张量、矢量和标量](# 标量、矢量、张量)处理单元，高效地处理深度学习中的大量并行计算任务。<img width="885" height="261" alt="image-20251011173114647" src="https://github.com/user-attachments/assets/a8d77586-5cc1-49a4-b10a-8de585d794e3" />

2. **AI CPU/Control CPU**：负责任务的调度、控制流管理以及非AI计算任务。
3. **片上高速缓存（On-chip Memory）**：靠近计算核心的高带宽存储器，用于存放中间结果，以最小化对外部内存的访问延迟。
4. **互联接口（Interconnect）**：包括**HCCS（Huawei Cache Coherency System）**用于多芯片之间的超高速互联，以及标准的PCIe接口。（[数据并行流程](# 数据并行（Data Parallelism）)）

二、 昇腾基础软件架构 **CANN（Compute Architecture for Neural Networks）** 是连接上层应用与底层硬件的关键桥梁。主要分为以下几个核心层级：

1. **应用层（Application Layer）** 💻：位于最上层，提供面向各种应用场景（如推理、训练、视频处理）的API和服务，方便开发者快速集成AI能力。

   主要是通过以下几种方式实现的：

   1. **统一的 Runtime 接口：** CANN 对上提供了一套统一的运行时 API 接口（Runtime API），开发者可以用这套接口来管理设备、内存、执行计算任务（无论是训练还是推理），而无需关心底层是哪一代昇腾芯片。
   2. **场景化加速库：** 针对特定的高频场景，如你提到的视频处理、图像识别、自然语言处理等，CANN 会封装好专门的**加速库**和**服务**。开发者可以直接调用这些功能库，避免了从零开始编写复杂的底层代码。
   3. **开发工具链：** 提供了模型转换、算子开发、性能分析和调优等一系列工具，将开发者在主流框架（如 PyTorch、TensorFlow）中训练好的模型，高效地部署到昇腾硬件上。

2. **图与运行时层（Graph & Runtime Layer）** ⚙️：这一层负责管理模型编译、图优化、资源调度和执行。它将深度学习框架（如PyTorch、TensorFlow）的模型转换为可直接在昇腾硬件上高效运行的图。

   主要包含两大核心功能：

   1. **模型转换与图优化 (Model Conversion & Graph Optimization)**：将上层框架的模型表示转换为昇腾芯片的统一计算图，并进行深度优化。
   2. **运行时管理与调度 (Runtime Management & Scheduling)**：负责芯片资源的分配、任务的调度和内存管理，确保计算任务高效执行。

3. **算子层（Operator Layer）** 🔢：位于最底层，包含了**昇腾算子库**，负责在 **Da Vinci AI Core** 上高效执行基础的AI计算操作（如卷积、池化、矩阵乘法等）。（[算力](# 算力)）

三、 具体项目例子

<img width="1108" height="607" alt="image-20251011102155050" src="https://github.com/user-attachments/assets/149d1db3-e89b-488c-934f-e2e5f348df4e" />

模型：代码 + 训练数据的结果。即：

1. 你用代码定义的**结构**。
2. 通过在海量数据上**训练**所学习到的**参数（Parameters）或权重（Weights）**。

##### 附加

###### 标量、矢量、张量<img width="905" height="352" alt="image-20251011173829158" src="https://github.com/user-attachments/assets/dff7d1eb-17af-4adb-a7b5-f1022fb0a635" />

######  数据并行（Data Parallelism）

当你有一个庞大的数据集（例如 10000 张图片）要用一个模型进行处理时，数据并行会将这个数据集分割成许多小块（例如每块 100 张图片），然后：

1. **复制模型：** 将**完整的模型**（结构和权重）复制到多个独立的计算单元（如多张昇腾 AI 芯片或多颗 AI Core）上。
2. **分配数据：** 将不同的小块数据分配给不同的计算单元。
3. **独立计算：** 每个计算单元独立地、并行地处理自己的那一份数据。

作用（训练场景）：

在训练中，这意味着每块芯片都计算自己的数据批次所产生的**梯度**（Gradient）。然后，通过高速互联接口（如 HCCS），将这些梯度汇总、平均，再更新到所有芯片上的模型权重中。

数据并行是最常用、最容易实现的并行训练方法，它直接加速了**处理大量数据**的速度。

###### 算力

**算力**是衡量一个硬件系统**执行计算任务能力**的指标。

核心定义：

- **算力**指的是单位时间内可以执行的**浮点运算次数**。
- 在 AI 领域，由于深度学习运算主要涉及大量的矩阵乘法（**张量计算**），因此算力通常用 **FLOPS（每秒浮点运算次数）** 来衡量。

<img width="399" height="249" alt="image-20251011174551135" src="https://github.com/user-attachments/assets/ce567f5d-53bc-4f8b-a3a0-c1077f5f082c" />



#### CANN环境搭建概要

搭建 CANN 环境是为了让操作系统、驱动程序和软件框架都能正确识别并使用昇腾硬件。主要步骤包括：

1. **硬件与驱动准备**：安装好昇腾加速卡，并安装对应的 **设备驱动程序**。
2. **CANN 软件包安装**：安装核心的 **CANN 软件包**。这包含了运行所有昇腾程序的底层库、运行时、算子库和 ATC 等工具。
3. **框架适配器安装**：安装如 **torch-npu**（用于 PyTorch）这样的适配器，作为主流框架和底层 CANN 的接口。

#### torch-npu 到底层 CANN 的调用栈

当你在 PyTorch 中执行一个训练或推理任务时，torch-npu 到 CANN 底层的调用栈可以分为以下几个关键层：

1. **上层应用（PyTorch）**：开发者调用标准的 PyTorch API（如 torch.add() 或 nn.Conv2d()）。
2. **框架适配层（torch-npu）**：它会**劫持（Hijack）**这些 PyTorch 调用，将标准的 CPU/CUDA 操作转换为昇腾设备上的操作。
3. **CANN 图与运行时层（Runtime/Graph）**：接收到 torch-npu 传递的昇腾操作请求后，**CANN Runtime** 负责调度 **AI Core** 资源、内存管理和任务流提交。
4. **CANN 算子层（Operator Layer）**：提供实际执行计算所需要的、高度优化的底层代码实现。
5. **驱动与硬件**：最终，CANN 通过设备驱动程序将任务下发到 **昇腾 AI Core** 上执行计算。

##### torch-npu

torch-npu 的意思是 **PyTorch for NPU**（即**神经网络处理单元**）。它的核心作用是：

1. **框架适配：** 它是连接主流深度学习框架 **PyTorch** 和 **昇腾 CANN 基础软件** 的桥梁。
2. **指令翻译：** 当你的 PyTorch 代码执行 Tensor 操作时，torch-npu 会**拦截（Hijack）**这些调用，并将标准的 PyTorch 操作（原本指向 CPU 或 CUDA）自动转化为对应的 **昇腾 NPU 操作**。

3. **设备管理：** 它让 PyTorch 能够识别和使用昇腾硬件设备。
