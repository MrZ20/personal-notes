这是一个关于在 **昇腾 (Ascend) NPU** 上运行 **Qwen2.5-VL-7B-Instruct** 模型精度测试的完整工作流的详细解释。该流程的核心是 **`pytest`** 驱动 **`lm-evaluation-harness`** 框架，通过 **`vllm-ascend`** 插件在 Ascend 硬件上进行高效推理。

以下是结合您提供的配置和文件路径的详细流程：

------



### 测试配置信息



| **参数**                 | **值**                          | **含义**                                                     |
| ------------------------ | ------------------------------- | ------------------------------------------------------------ |
| **`model_name`**         | `"Qwen/Qwen2.5-VL-7B-Instruct"` | 指定要加载的**多模态大语言模型 (MLLM)**。                    |
| **`model`**              | `"vllm-vlm"`                    | 指定 `lm-eval` 使用的模型后端。`vllm-vlm` 特指支持多模态（VLM - Vision-Language Model）的 vLLM 后端。 |
| **`tasks`**              | `"mmmu_val"`                    | 指定要运行的精度评估任务。`mmmu_val` 是一个多模态（Multiple-Choice Multimodal Understanding）基准测试，要求模型处理图像和文本。 |
| **`acc,none` / `value`** | `0.51`                          | 预期的精度指标（这里是 `mmmu_val` 任务的准确率），用于最终的断言检查。 |
| **`max_model_len`**      | `8192`                          | 设置 vLLM 引擎的最大上下文长度。                             |

------



### 整体流程详解

#### 步骤 1: 配置加载



| **文件/模块**                     | **描述**           |
| --------------------------------- | ------------------ |
| **`test_lm_eval_correctness.py`** | **主导文件。**     |
| **`Qwen2.5-VL-7B-Instruct.yaml`** | **输入配置文件。** |
| **`conftest.py`**                 | **解析工具。**     |

1. **命令行启动：** `pytest` 运行，并将配置文件的路径传递给 `test_lm_eval_correctness_param` 函数（通过 `config_filename` Fixture）。
2. **YAML 解析：** 测试函数使用 `yaml.safe_load(config_filename.read_text(...))` 读取并解析 `Qwen2.5-VL-7B-Instruct.yaml` 文件，将其转换为 Python 字典 (`eval_config`)。
3. **参数提取：** 提取所有配置参数，特别是模型名称、并行参数（如果配置中存在）和任务列表。



#### 步骤 2: 引擎初始化



| **文件/模块**                 | **描述**       |
| ----------------------------- | -------------- |
| **`vllm_ascend/engine/\*`**   | **引擎核心。** |
| **`vllm_ascend/worker/\*`**   | **模型加载。** |
| **`vllm_ascend/platform.py`** | **硬件注册。** |

1. **参数构建：** `test_lm_eval_correctness.py` 调用 `build_model_args` 函数，将配置参数（如 `max_model_len=8192`）转换为 vLLM 期望的格式。
2. **LLM 实例化：** 在 `lm-eval` 内部（或其调用的测试逻辑中），使用 `vllm.LLM(model="Qwen/Qwen2.5-VL-7B-Instruct", ...)` 实例化推理引擎。
3. **Ascend 平台识别：** vLLM 核心检测到 `vllm-ascend` 插件的存在，并使用 `vllm_ascend/platform.py` 中定义的接口，确保所有后续操作都针对 Ascend NPU 进行。
4. **模型权重加载：** `AscendWorker` (在 `vllm_ascend/worker/*`) 负责：
   - **下载/加载** Qwen2.5-VL-7B-Instruct 的权重。
   - **张量并行 (TP) 初始化：** 如果配置了 `tensor_parallel_size` > 1，工作进程在多个 NPU 之间协调，分割模型权重。
   - **KV Cache 分配：** 根据 `max_model_len` 和 NPU 内存情况，分配并初始化 PagedAttention 所需的 KV Cache 内存块。



#### 步骤 3: 任务集成

| **文件/模块**    | **描述**       |
| ---------------- | -------------- |
| **`lm_eval` 库** | **评估框架。** |

1. **构建评估参数：** 测试脚本将模型参数、任务列表 (`tasks=["mmmu_val"]`) 以及后端类型 (`model="vllm-vlm"`) 打包成字典 (`eval_params`)。
2. **调用评估 API：** 执行 `results = lm_eval.simple_evaluate(**eval_params)`。
3. **`vllm-vlm` 后端激活：** `lm-eval` 框架识别到 `vllm-vlm` 模式，它会使用预先实例化的 vLLM 引擎（或在内部创建）作为其推理后端。



#### 步骤 4: 数据加载

| **文件/模块**    | **描述**         |
| ---------------- | ---------------- |
| **`lm_eval` 库** | **数据集管理。** |

1. **任务加载：** `lm-eval` 框架根据任务名 `"mmmu_val"`，自动定位并加载该基准测试所需的数据集（包括图像和文本样本）。
2. **格式化：** 数据集中的每个样本（通常是 **多模态数据**，即图像路径/数据 + 文本提示 + 标准答案）都会被加载和预处理。



#### 步骤 5: 运行推理 (核心 NPU 计算)

| **文件/模块**               | **描述**       |
| --------------------------- | -------------- |
| **`lm_eval` 库**            | **请求循环。** |
| **`vllm_ascend/worker/\*`** | **推理执行。** |
| **`NPU驱动` / `torch-npu`** | **硬件交互。** |

1. **循环推理：** `lm-eval` 开始遍历 `mmmu_val` 数据集，并将批量的多模态请求（`prompts`）发送给 vLLM 引擎。
2. **请求调度：** vLLM 引擎的 **调度器** (可能由 `vllm_ascend/scheduler/ascend_scheduler.py` 优化) 接收请求，将序列分块并放入批次。
3. **NPU 计算：**
   - `AscendModelRunner` 接收数据，并协调模型在 NPU 上的运行。
   - 对于 Qwen2.5-VL 模型，它首先将图像输入通过 **视觉编码器 (Vision Encoder)**。
   - 然后，将视觉特征和文本 Token 组合，输入到 **LLM 解码器**中。
   - **Ascend 优化：** 计算过程中涉及的注意力（Attention）、MLP、MoE（如果模型支持）等操作，都通过 `torch-npu` 或 vllm-ascend 的 **自定义算子**（Custom Ops）在 NPU 上高效执行，可能以 **ACLGraph** 模式运行以提高性能。
4. **结果返回：** 引擎生成最终的文本输出（模型的预测答案），并将其返回给 `lm-eval`。



#### 步骤 6: 结果验证



| **文件/模块**                     | **描述**         |
| --------------------------------- | ---------------- |
| **`test_lm_eval_correctness.py`** | **断言逻辑。**   |
| **`numpy`**                       | **浮点数比较。** |

1. **指标计算：** `lm-eval` 比较模型的输出和数据集的标准答案，计算出 **实际的精度指标**（例如 `measured_value`）。

2. **断言检查：** 测试函数执行以下检查：

   Python

   ```
   task_success = bool(np.isclose(ground_truth, measured_value, rtol=RTOL))
   assert success
   ```

   - 将实际精度与配置文件中的预期值 `0.51` 进行比较。
   - 使用 `np.isclose` 和容忍度 `RTOL` (0.05) 确保结果在可接受的浮点误差范围内。

3. **报告生成：** 调用 `generate_report` 函数，将这次测试的所有信息、环境版本和精度结果（✅ 或 ❌）写入 Markdown 报告文件。

4. **测试结束：** 如果所有任务的精度都符合预期，`pytest` 测试成功；否则，测试失败。



### 附加

#### 推理引擎

职责：负责**高效、快速、低延迟**地运行一个**预训练好的模型**（如 LLM）。它管理硬件资源、数据流和计算过程

示例：vLLM 引擎、TensorRT-LLM 引擎、OpenVINO 引擎

##### vllm

目标：在不牺牲模型精度的前提下，最大限度地提高 LLM 的吞吐量（Throughput）和效率，并降低延迟（Latency）

vLLM 关键技术：**PagedAttention**：

- **传统问题：** 传统的 LLM 推理中，Key 和 Value 缓存（KV Cache）的内存分配是连续的，会导致内存碎片化，限制了同时处理的请求数量。
- **vLLM 解决方案 (PagedAttention)：** vLLM 借鉴了操作系统中的**分页（Paging）概念。它将 KV Cache 存储在非连续的内存块中（称为块 Block**），通过一个**块表（Block Table）**来映射这些块。
  - **优点：** 极大地减少了内存碎片和浪费，使得 GPU/NPU 内存可以更有效地用于存储 KV Cache，从而**显著增加服务器能够同时处理的请求数量（即提高吞吐量）**。

#### LLM 实例化

创建一个可以立即用于推理和生成文本的 vLLM 推理引擎对象

在 vLLM 的语境中，**LLM 实例化**指的是：

| **概念**                         | **含义**                                                     |
| -------------------------------- | ------------------------------------------------------------ |
| **类（Class）**                  | **`vllm.LLM`**。这是一个 Python 类，定义了 vLLM 引擎如何工作、需要哪些参数（如模型名称、并行度、内存利用率）以及提供哪些方法（如 `generate()`）。 |
| **对象/实例（Object/Instance）** | **`llm = LLM(...)`** 这一行代码执行后的结果 `llm`。它是 `vllm.LLM` 类的具体运行实体。 |
| **实例化（Instantiation）过程**  | 当你执行 `llm = LLM(model="Qwen/Qwen2.5-VL-7B-Instruct", ...)` 时，系统执行以下操作： 1. **分配内存：** 为 LLM 引擎和其内部组件（如调度器、KV Cache）分配内存。 2. **加载模型：** 从 Hugging Face 或 ModelScope 下载/加载 Qwen2.5-VL-7B-Instruct 的模型权重。 3. **初始化硬件：** 根据配置（例如在 `vllm-ascend` 中），在指定的 NPU 上初始化设备环境，并进行必要的图编译（如 ACLGraph 模式）。 4. **就绪：** `llm` 对象现在是“热启动”状态，可以立即接收 Prompt 并开始生成文本。 |

#### 权重加载（Weight Loading）

**含义：** 指将预训练好的模型参数（即模型的“权重”）从磁盘存储（如硬盘或固态硬盘）加载到计算设备（如 GPU 或 **昇腾 NPU**）的高速内存（如 HBM）中的过程。

| **概念**              | **作用**                                                     | **在 LLM 中的重要性**                                        |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **模型权重**          | 模型的所有学习到的参数，例如数千个矩阵和向量，它们定义了模型如何将输入转换为输出。一个 70 亿参数的模型，即使使用 16 位浮点数（BF16/FP16），其权重大小也约为 **14 GB**。 | **基础。** 只有将模型权重加载到 NPU 内存中，模型才能执行前向计算（推理）。这是推理引擎启动的第一个、也是最耗时的步骤之一。 |
| **在 vLLM-Ascend 中** | `vllm_ascend/worker/*` 模块负责协调这一加载过程，确保权重正确地从磁盘或模型缓存中读取，并传输到昇腾 NPU 的显存（HBM）。 |                                                              |

#### 张量并行（Tensor Parallelism, TP）

**含义：** 一种**模型并行（Model Parallelism）\**技术，用于将\**单个大型模型**的权重和计算**切分**到多个计算设备（NPU/GPU）上。

| **概念**              | **作用**                                                     | **举例**                                                     |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **切分模型**          | 当一个模型的权重太大，单个 NPU 内存无法容纳时（例如一个 70B 的模型需要 140 GB 内存，而单个 NPU 只有 32 GB），就需要使用 TP。 | 将模型中巨大的矩阵乘法运算（如 Transformer 层中的 `W_q, W_k, W_v, W_o` 矩阵）按列或按行切分成 $N$ 份。 |
| **并行计算**          | 每个 NPU 只存储和计算模型的一部分权重。在推理过程中，设备之间需要通过高速通信接口（如 NPU 间的互联或 NCCL/HCCL）**交换激活值**，以确保数学运算的正确性。 | 如果 TP=4，一个 7B 模型被切分成 4 份，每份模型只占用大约 1/4 的内存（不包括 KV Cache），计算速度也会提升。 |
| **在 vLLM-Ascend 中** | `tensor_parallel_size` 参数决定了切分份数。`vllm-ascend` 需要使用昇腾平台特有的 **HCCL**（华为集合通信库）或类似的通信机制来高效地进行并行数据交换。 |                                                              |

#### KV Cache 分配（Key-Value Cache Allocation）

**含义：** 指在 LLM 推理过程中，为存储 Attention 机制中的 **Key（K）**和 **Value（V）**向量（即 **KV Cache**）分配和管理内存。

| **概念**              | **作用**                                                     | **在推理中的重要性**                                         |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **KV Cache**          | 在生成每个新 Token 时，模型会计算当前 Token 与**之前所有 Token** 的 K 和 V 向量。为了避免重复计算前面所有 Token 的 K/V 向量，这些值会被缓存起来。 | **加速解码（Decoding）过程。** 如果没有 KV Cache，每个新 Token 的生成时间会随着序列长度的增加而线性增加。 |
| **KV Cache 分配**     | KV Cache 通常占用大量的显存（HBM）。其内存需求取决于**批次大小（Batch Size）和序列最大长度（Max Sequence Length）**。 | KV Cache 的有效管理是 vLLM **PagedAttention** 技术的核心。vLLM 不会一次性分配巨大的连续内存，而是将内存划分为小块（Blocks），按需分配给不同的请求序列。 |
| **在 vLLM-Ascend 中** | vLLM 引擎的调度器（Scheduler）和内存管理器负责在昇腾 NPU 的 HBM 中**动态地**分配和释放 KV Cache 内存块，从而在相同的硬件资源下，允许更多的请求同时驻留，提高了吞吐量。 |                                                              |

####  批次大小（Batch Size）

**含义：** 指在一次推理迭代中，**同时输入给模型进行处理的独立请求（或序列）的数量。**

| **特点**       | **详细解释**                                                 |
| -------------- | ------------------------------------------------------------ |
| **同步处理**   | 批次中的所有请求会作为一个大的数据块，在 NPU/GPU 上**并行处理**。 |
| **吞吐量影响** | **批次大小越大，模型的吞吐量（每秒处理的 Token 数）通常越高。** 这是因为并行处理效率更高，并且可以更好地利用硬件资源。 |
| **延迟影响**   | **批次大小过大，可能会增加延迟。** 当批次中的最慢请求（最长的序列）完成计算后，整个批次才能释放，导致短序列必须等待长序列。 |
| **内存影响**   | **批次大小越大，KV Cache 占用的内存总量越大。** 这是因为每个序列都需要自己的 KV Cache 空间。这是 LLM 推理中最主要的内存限制因素。 |
| **在 vLLM 中** | vLLM 使用 **动态批处理（Dynamic Batching）**。它不是提前固定 Batch Size，而是在运行时尽可能地将所有待处理的请求组合成一个批次，最大化硬件利用率。 |

#### 序列最大长度（Max Sequence Length）

**含义：** 指模型在单次推理中**能够处理的输入 Prompt 和生成输出 Token 的总和的最大长度**（以 Token 计）。

| **特点**                   | **详细解释**                                                 |
| -------------------------- | ------------------------------------------------------------ |
| **模型限制（上下文窗口）** | LLM 都有一个固定的**上下文窗口（Context Window）**大小，这是模型设计时决定的。`Max Sequence Length` 不能超过这个窗口大小。例如，Qwen2.5-VL-7B-Instruct 的配置中是 8192。 |
| **KV Cache 影响**          | **序列最大长度是 KV Cache 内存分配的关键决定因素。** 无论序列当前多长，推理引擎通常需要预留**最坏情况**下的 KV Cache 空间，即直到达到这个最大长度所需的空间。 |
| **推理延迟**               | 序列越长，模型在每个生成步骤中需要进行的 Attention 计算量越大（因为它要关注前面所有 Token 的历史信息），导致**延迟增加**。 |
| **配置目的**               | 在您的配置中，`max_model_len: 8192` 告诉 vLLM 引擎，它需要为任何一个请求序列预留最大 8192 个 Token 的 KV Cache 空间。 |