#### vLLM插件化机制及特性

##### 定义和目的

插件化：将应用程序的核心和扩展功能分离

插件化设计典型库：Pytorch/Paddle

vLLM 插件化适配最主要的目的和优势：

- **模块化和解耦：** 不同的硬件平台（如 NVIDIA CUDA、AMD ROCm、各种 NPU 或定制化加速器）可以作为独立的插件进行开发和维护。vLLM 核心开发者无需深入了解每个硬件后端的细节，使vLLM核心代码更简洁。
- **快速集成：** 新的硬件厂商可以独立开发其执行器（Executor）、工作器（Worker）、模型运行器（Model Runner）等组件，并通过插件机制快速适配 vLLM 框架，大幅缩短上市时间。插件化后，硬件后端看护从”中心式“变为”分布式“。
- **降低维护负担：** 核心 vLLM 代码与硬件相关的实现分离。vLLM 开发者可以专注于通用功能的优化（如 PagedAttention、动态批处理），而硬件厂商则负责维护各自的插件。

##### 举例

1. **中心式 CI（非插件化）的比喻**

在没有插件化的年代，**手机系统**和**芯片驱动**是高度捆绑、不分离的。

- **流程：** Google（核心团队）在开发新版安卓系统时，必须把所有已知芯片厂商（高通、联发科等）的驱动代码**全部打包**到自己的系统代码仓库里。
- **看护模式（中心式）：** Google 每次修改一行安卓代码，都必须在自己的测试基地里，找齐所有芯片：高通手机、联发科手机... 全部运行一次**全量系统测试**，确保所有芯片都能正常工作。
  - **痛点：**
    1. **资源压力大：** Google 要购买和维护所有厂商的芯片测试机。
    2. **效率低下：** 任何一个小改动，都需要等待所有芯片的漫长测试。
    3. **责任混淆：** 如果联发科的驱动代码出错了，是 Google 的 CI 失败了，Google 还要去通知联发科来修复。

2. **分布式 CI（插件化）的比喻**

vLLM 采用插件化后，核心框架和硬件后端解耦，变成了**分布式 CI**。

- **流程：**
  1. **Google (vLLM 核心) 职责：** 只维护安卓系统的核心代码和接口（例如，如何调用相机、如何使用屏幕）。
  2. **高通 (插件) 职责：** 独立维护和发布**高通芯片驱动**，它是一个独立可下载的模块。
- **看护模式（分布式）：**
  1. **核心框架 CI (Google 的 CI)：** Google 只测试安卓系统的通用功能。它只验证其**接口规范**是否正确。
  2. **硬件插件 CI (高通的 CI)：** 高通自己有一个**独立的 CI 系统**。这个 CI 系统只做一件事：**测试最新的高通驱动在最新版安卓系统上运行是否正确。** 高通自己拥有自己的芯片测试机。

**优势：**

| **分布式 CI 优势** | **比喻说明**                                                 |
| ------------------ | ------------------------------------------------------------ |
| **独立迭代**       | Google 可以每天发布安卓系统的 Beta 版，高通可以每周发布一次驱动更新，两者互不影响，各自的 CI 互不依赖。 |
| **专业看护**       | 只有高通工程师需要了解高通芯片的复杂细节，他们对自己的驱动 CI 失败负全责，无需 Google 介入。 |
| **效率提升**       | Google 的核心 CI 只需要测试通用代码，测试速度更快，极大地加快了核心框架的开发节奏。 |

##### 兼容性问题

即使有了插件化和分布式 CI，版本兼容性问题仍然存在，这是**接口契约（Interface Contract）**固有的风险。

兼容性问题通常发生在以下两种情况：

**情况一：核心框架的接口发生了“破坏性变更”**

- **技术上：** vLLM 核心团队为了提升性能，决定重构一个关键接口（比如，如何管理 KV Cache 内存的接口）。
- **比喻：** Google 决定修改安卓系统**调用相机硬件**的方式，比如将 `call_camera()` 接口改成了 `start_camera_stream()`。
- **结果：** 此时，高通插件的 CI 就会失败。因为高通的驱动代码还在调用旧的 `call_camera()`，但新版安卓（vLLM）已经没有这个接口了。
- **解决：** 高通（插件开发者）必须更新其驱动代码，**适配**这个新的接口，并在其独立的 CI 中验证通过后，发布新的插件版本。

**情况二：插件的实现与核心框架的假设不符**

- **技术上：** 某个硬件插件在实现 vLLM 的 `Attention` 机制时，依赖了某个特定的 CUDA 版本特性。
- **比喻：** 联发科在新驱动中使用了某个**特定的图形渲染技巧**，该技巧在安卓系统 12 上没问题，但在安卓系统 13 上，Google 优化了渲染器，导致这个技巧不再生效，从而引发画面闪烁。
- **结果：** 只有在高通（插件开发者）自己的 CI 中，或者用户实际部署时，才会发现问题。这个问题需要由插件开发者修复。

**核心结论：** 插件化**分离了维护和测试的责任**，但**没有消除不同代码模块之间接口依赖**。版本兼容性问题就是这种依赖关系被破坏时发生的。因此，分布式 CI 的一个重要环节就是**定期进行集成测试**，确保核心框架和已发布的插件版本之间仍然能协同工作。

##### 已有插件化组件

在 vLLM 的分布式架构中，一个请求从接收到处理完成，会依次经过以下组件，其中许多组件都设计成了可插拔（Pluggable）的接口，特别是对于新的硬件后端：

1. **Worker (工作器)**

- **角色定位：** Worker 是负责在特定硬件设备上执行**实际计算任务**的进程或线程。它是模型推理的执行单元。
- **核心职责：**
  - 管理和维护 **ModelRunner** 的实例。
  - 接收由 **Executor**（执行器/调度器）分配的任务和批次。
  - 协调 **ModelRunner** 进行 Prefill（预填充）和 Decode（解码）步骤。
- **插件化意义：** 在插件化架构中，每个**平台插件**（如 Ascend NPU 插件）都需要提供一个定制的 `Worker` 实现，以确保任务能够被正确地分配和执行到该硬件上。



2. **ModelRunner (模型运行器)**

- **角色定位：** ModelRunner 是直接与模型权重和底层张量操作交互的组件。它封装了模型的正向传播逻辑。
- **核心职责：**
  - 加载并管理模型的权重（Weights）。
  - 执行模型的**预填充 (Prefill)** 和**解码 (Decode)** 操作。
  - 与 **Attention Backend** 交互，执行高效的注意力计算。
  - 管理 **KV Cache** 的读写。
- **插件化意义：** 不同的硬件平台（如 NPU）通常需要定制化的模型加载和运行方式（可能涉及特定的编译器或运行时库），因此插件需要提供定制的 `ModelRunner` 来适配这些底层差异。



3. **Attention (注意力后端)**

- **角色定位：** Attention Backend 是 vLLM 性能核心 **PagedAttention** 机制的具体实现者。
- **核心职责：**
  - 高效计算 Transformer 模型中的**自注意力 (Self-Attention)** 机制。
  - 管理 KV Cache（Key-Value Cache）的**分页、分块和共享**，这是实现高吞吐和内存效率的关键。
- **插件化意义：** 硬件适配的关键在于能否高效实现 PagedAttention。对于新的硬件（如 AMD 或 NPU），插件必须提供一个定制的 **Attention Backend** 实现，通常需要利用该硬件的定制加速核或优化指令集来最大化计算性能。



4. **Communicator (通信器)**

- **角色定位：** 在分布式推理（如张量并行 Tensor Parallelism）场景下，Communicator 负责不同设备（GPU 或 NPU）之间的数据和梯度通信。
- **核心职责：**
  - 执行诸如 `all-reduce`、`all-gather`、`broadcast` 等并行通信操作，确保模型权重和激活值在多设备之间正确同步。
- **插件化意义：** 不同的硬件和集群环境（如 NVIDIA NCCL、AMD RCCL、或专门的 NPU 通信库）有不同的通信后端。平台插件需要提供定制的 `Communicator` 来利用该平台最高效的通信原语。



5. **Custom Op (自定义操作)**

- **角色定位：** Custom Op 是指那些不属于标准深度学习框架（如 PyTorch 或 TensorFlow）的、但对于特定模型或硬件至关重要的定制化底层操作。
- **核心职责：** 通常用于实现高度优化的内核函数（Kernels），如融合操作（Fused Operations）、定制的激活函数、或针对特定量化格式的张量计算。
- **插件化意义：** 当集成一个新的硬件后端时，为了获得最佳性能，插件开发者常常需要编写 **Custom Op** 来替代 vLLM 中的标准 PyTorch 操作，通过利用硬件的底层特性来实现加速。



6. **Prefilling (预填充/前向阶段)**

- **角色定位：** Prefilling 是指 LLM 推理的第一个阶段，即处理用户的**完整输入 Prompt** 并生成第一个输出 Token 之前的计算过程。
- **核心职责：**
  - 计算输入的 Key-Value 向量。
  - 将生成的 KV 向量写入 **KV Cache**。
  - 计算并输出第一个 Token 的 Logits。
- **与插件的关系：** Prefilling 是一个**计算密集型 (Compute-Bound)** 阶段，ModelRunner 和 Custom Op 的优化主要体现在这一阶段。vLLM 通过**Chunked Prefill**等技术来优化这一阶段的延迟。



7. **Plugin (插件)**

- **角色定位：** Plugin 是一个顶层概念，是可**动态加载**的外部 Python 包或模块。
- **核心职责：** 插件系统是 vLLM 实现**可扩展性**和**责任分散**的基石。它通过 Python 的 `entry_points` 机制来识别和加载外部代码。
- **分类：**
  - **平台插件 (`vllm.platform_plugins`)：** 用于注册上述所有的硬件定制组件（Worker, ModelRunner, Attention, Communicator）。
  - **通用插件 (`vllm.general_plugins`)：** 用于注册树外模型等。
  - **I/O 插件等：** 用于定制预处理和后处理。

------

总而言之，**Plugin** 提供了将外部代码集成到 vLLM 的机制，而 **Worker、ModelRunner、Attention 和 Communicator** 则是插件需要替换或定制的**核心运行时接口**，以确保 LLM 推理能够在新的硬件上高效运行。

##### vLLM插件机制

**“Platform”** 在 vLLM 中是一个**抽象的概念**，代表了某种特定的硬件运行环境（如 NVIDIA CUDA 或 Ascend NPU）。 **“Plugin”** 则是**实现**这个抽象的**具体代码包**。

 **vLLM 插件化适配**中 **Platform (平台)**、**Plugin (插件)** 和 **`entry_points` 机制**之间的关系：

| **概念**                                         | **角色定位**                             | **核心作用**                                                 |
| ------------------------------------------------ | ---------------------------------------- | ------------------------------------------------------------ |
| **Plugin (插件)**                                | **具体的代码包（例如 `vllm-ascend`）。** | 是**第三方开发者**为特定硬件或功能提供的**可安装、可替换的实现**。它包含了一系列定制化的类和函数。 |
| **Platform (平台)**                              | **核心框架的抽象接口或容器。**           | **抽象概念。** 它定义了运行环境（如 NVIDIA/Ascend）需要具备哪些组件接口。它充当着一个**“工厂”**，负责提供正确的 Worker、ModelRunner 等实现。 |
| **Worker, ModelRunner, Attention, Communicator** | **底层的运行时功能组件。**               | 它们是模型推理和硬件加速的**具体执行者**。它们是 **Platform 接口必须提供和返回的对象**。 |

##### Platform 与 Plugin 的配合机制

Platform 和 Plugin 的配合，是通过 Python 的 `entry_points` 机制实现的**松耦合和动态发现**：

1. **定义抽象 (Platform)：** vLLM 核心库定义了 `Platform` 接口，要求任何硬件后端必须实现和提供一套完整的运行时组件（Worker、ModelRunner 等）。
2. **实现功能 (Plugin)：** 硬件厂商（如 Ascend）将自己定制的组件代码打包成一个独立的 **Plugin** 包（例如 `vllm-ascend`）。
3. **注册入口 (`entry_points`):** 厂商在 `vllm-ascend` 包的配置中，使用 `entry_points` 机制，声明：**“我这个包里有 Platform 的实现，注册到 `vllm.platform_plugins` 这个槽位。”**
4. **动态发现 (vLLM 核心启动):**
   - vLLM 核心启动时，它通过查找所有已安装包的 `entry_points`，来**动态发现**有哪些插件注册了 `vllm.platform_plugins`。
   - vLLM 核心无需知道这些插件叫什么名字，只要发现它们，就加载并执行它们的注册函数。
5. **加载与使用：** 被加载的插件（Platform 实现）将自己的定制组件（如 `AscendWorker`）注入到 vLLM 的运行时环境中，从而实现硬件的插件化适配。



##### Python `entry_points` 机制

`entry_points` 是一种让一个 Python 包（宿主程序）能够**发现并加载**由**其他独立安装的 Python 包（插件）**提供的特定代码或功能的方式，而无需在宿主代码中显式引用这些插件。是Python 包管理系统（如 `setuptools` 或 `flit`）提供的一种机制，用于实现**插件架构**：

- **作用：** 允许一个 Python 包（宿主）在运行时发现并加载由**其他独立安装的包**提供的特定代码。
- **机制：** 它通过在插件包的打包元数据（Metadata）中定义一个**组名**和**代码路径**来实现注册。
- **优势：** 实现了宿主程序和插件之间的**代码分离**和**零配置自动加载**，极大地提高了系统的模块化和可扩展性。

如何工作？

`entry_points` 的定义通常位于插件包的配置文件中，例如 `setup.py` 或 `pyproject.toml`：

**示例：vLLM Ascend 插件的配置文件片段**

```toml
# 假设这是 vllm-ascend 插件的 pyproject.toml 文件内容

[project.entry-points."vllm.platform_plugins"]
# 这里的 "ascend" 是插件名称
# 右边是当 vLLM 核心发现这个插件时，需要调用的 Python 函数路径
ascend = "vllm_ascend.platform:register_platform"
```

1. **插件注册（定义）：** 上述配置告诉 Python 打包系统：在 `vllm.platform_plugins` 这个**组（Group）**中，有一个名为 `ascend` 的插件，它的入口是 `vllm_ascend.platform` 模块里的 `register_platform` 函数。

2. **核心发现（加载）：** 当用户安装了这个插件后，vLLM 核心启动时，会运行类似如下的 Python 代码：

   ```python
   import importlib.metadata
   
   # 查找所有注册在 vllm.platform_plugins 组下的入口点
   plugins = importlib.metadata.entry_points(group='vllm.platform_plugins')
   
   for entry_point in plugins:
       # 核心框架加载并运行插件的注册函数
       loaded_function = entry_point.load() 
       loaded_function() # 此时 vLLM Ascend 的平台代码被注入
   ```

**优势：**

- **真正的解耦：** vLLM 核心代码完全不知道世界上是否存在名为 `vllm-ascend` 的包。它只知道有一个叫做 `vllm.platform_plugins` 的插槽。
- **零配置加载：** 用户只需要 `pip install vllm-ascend`，插件就会自动被 Python 打包系统注册到 `entry_points`，vLLM 核心在启动时就能自动发现并加载它。
- **模块化管理：** 硬件厂商可以独立维护、发布和升级他们的插件包，不依赖 vLLM 核心的版本发布周期。

#### 并行策略（Parallelization Strategies）

##### 1. DP：数据并行 (Data Parallelism)

- **核心思想：** 每个设备（Worker）都拥有**完整的模型副本**，但处理**不同的数据批次**。
- **如何工作：**
  1. 将输入数据的批次（Batch）切分成 $N$ 份，分配给 $N$ 个设备。
  2. 每个设备独立地进行前向传播和反向传播。
  3. **关键步骤：** 在反向传播后，需要使用 **All-Reduce** 等通信操作，将所有设备的梯度**平均同步**到一起，然后每个设备再用同步后的平均梯度更新自己的模型副本。
- **适用场景：** 模型太大，但可以被单个设备容纳，但**数据量极大**，需要加速训练。
- **优点：** 实现简单，通信开销相对较低（只同步梯度）。
- **缺点：** 无法解决**模型内存限制**问题（每个设备仍需存储完整模型）。

##### 2. TP：张量并行 (Tensor Parallelism)

- **核心思想：** 将模型的**单个张量（权重矩阵）**切分到多个设备上，每个设备只存储和计算张量的一部分。
- **如何工作：**
  1. 将 Transformer 层中的关键矩阵（如 Self-Attention 或 FFN 层的权重）沿某一维度进行切分。
  2. 每个设备只计算输入和自己那一小部分权重的乘积。
  3. **关键步骤：** 在某些操作（如矩阵乘法或激活函数）之前或之后，需要进行 **All-Gather** 或 **All-Reduce** 等通信操作，同步中间激活值。
- **适用场景：** **模型太大**，单个设备内存不足以容纳完整的模型参数。
- **优点：** 解决了模型的内存瓶颈。
- **缺点：** 通信开销大，需要在每层模型内部进行多次设备间通信。
- **在 vLLM 中的体现：** vLLM 使用 TP 来部署大型模型进行推理，实现单次请求的低延迟。

##### 3. PP：流水线并行 (Pipeline Parallelism)

- **核心思想：** 将模型的**层（Layer）**切分到不同的设备上，形成一个处理数据的流水线。
- **如何工作：**
  1. 模型的前 $L_1$ 层放在设备 A，接下来的 $L_2$ 层放在设备 B，以此类推。
  2. 设备 A 完成计算后，将**激活值**（Activation）传输给设备 B，设备 B 基于此继续计算，像工厂流水线一样。
  3. 为了提高设备利用率，输入数据通常也被切分成 **微批次 (Micro-Batches)**，使得当设备 B 处理微批次 $i$ 时，设备 A 可以开始处理微批次 $i+1$。
- **适用场景：** **模型层数极多**，需要最大限度地减少通信次数（只在层与层之间通信）。
- **优点：** 相对 TP，通信开销更少（只传输激活值），能处理超大模型。
- **缺点：** 存在**气泡时间 (Bubble Time)**，即流水线开始和结束时，部分设备处于空闲状态，降低了整体效率。

##### 4. EP：专家并行 (Expert Parallelism)

- **核心思想：** 专为 **MoE 模型 (Mixture of Experts)** 设计的并行策略，用于解决专家网络数量过多的问题。
- **如何工作：**
  1. 将 MoE 层中的**所有专家 (Experts)** 分布到不同的设备上。
  2. 模型的其他部分（非 MoE 层）可能采用 DP 或 TP 策略。
  3. **关键步骤：** 当 Token 到达 MoE 层时，门控网络（Router）会决定将 Token 路由到哪个设备上的哪个专家。这个过程涉及到高效的 **All-to-All** 通信，用于将 Token 分发给正确的专家，再收集结果。
- **适用场景：** 部署 **MoE 架构**的模型（如 Mixtral、DeepSeek MoE）。
- **优点：** 能够处理具有巨大总参数量（数万亿）的模型。
- **在 vLLM Ascend 中的体现：** Ascend 插件的关键就在于通过定制的 `Communicator` 和 `Custom Op` 来高效实现 MoE 的 **All-to-All** 通信和稀疏计算，这是实现 EP 的技术核心。

**总结对比表**

| **策略**            | **划分对象** | **解决的主要瓶颈**             | **关键通信操作**                     | **vLLM 关键应用**                  |
| ------------------- | ------------ | ------------------------------ | ------------------------------------ | ---------------------------------- |
| **DP (数据并行)**   | 训练数据     | **数据量**过大，训练速度慢     | All-Reduce (梯度同步)                | 训练阶段，非推理优化重点           |
| **TP (张量并行)**   | 单层权重矩阵 | **单卡内存**不足以存储模型     | All-Gather / All-Reduce (激活值同步) | **核心应用**：部署大模型推理       |
| **PP (流水线并行)** | 模型层数     | **层数极多**，超大模型分层存储 | 点对点通信 (激活值传输)              | 部署超大模型推理，平衡延迟与吞吐   |
| **EP (专家并行)**   | MoE 专家网络 | **专家数量**庞大，稀疏路由计算 | All-to-All (Token 路由/分发)         | **Ascend 插件重点**：部署 MoE 模型 |